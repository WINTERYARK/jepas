{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Joint Embedding Predictive Architecture\n",
        "\n",
        "* [JEPA](https://arxiv.org/pdf/2306.02572)"
      ],
      "metadata": {
        "id": "Cp8pQqZX5exg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ],
      "metadata": {
        "id": "gx1N-T7v5sK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install x-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6NsO7XB5ghd",
        "outputId": "f38d0593-c4f4-4f5f-d1e7-b8cb8f054f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: x-transformers in /usr/local/lib/python3.10/dist-packages (1.44.4)\n",
            "Requirement already satisfied: einx>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from x-transformers) (0.3.0)\n",
            "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from x-transformers) (0.8.0)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from x-transformers) (0.7.3)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from x-transformers) (24.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from x-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from einx>=0.3.0->x-transformers) (1.26.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from einx>=0.3.0->x-transformers) (1.13.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.10/dist-packages (from einx>=0.3.0->x-transformers) (2.4.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->x-transformers) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->x-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->x-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->x-transformers) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->x-transformers) (2024.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->einx>=0.3.0->x-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->x-transformers) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "qRNtYG0Z5dO5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc__nJml5G6O"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from typing import *\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from x_transformers import Encoder, Decoder\n",
        "from x_transformers.x_transformers import ScaledSinusoidalEmbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokeniser\n",
        "\n",
        "* Tokenisers split words into chunks to be processed individually be a language model.\n",
        "* We employ subword tokenisation using BERT: [explanation](https://h2o.ai/wiki/bert/#:~:text=BERT%2C%20short%20for%20Bidirectional%20Encoder,framework%20for%20natural%20language%20processing.), [paper](https://arxiv.org/abs/1810.04805). (See example below.)"
      ],
      "metadata": {
        "id": "M_5B1cje5wEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "RSjMAjFj5y8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained BERT tokeniser\n",
        "tokeniser = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Rb_kCb5ws7",
        "outputId": "cc92d0a0-ddf0-4a90-b0f0-cd32d36b21f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Tokenisation Example"
      ],
      "metadata": {
        "id": "9EURnuDV61ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text: str = \"This is an example sentence for subword tokenization.\"\n",
        "print(f\"{text=}\")\n",
        "\n",
        "# Tokenize a sentence\n",
        "tokens: List[str] = tokeniser.tokenize(text)\n",
        "print(f\"{tokens=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TCeujaf6xuc",
        "outputId": "ea2ece4e-6435-4363-a4b2-c49f688bfa65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='This is an example sentence for subword tokenization.'\n",
            "tokens=['this', 'is', 'an', 'example', 'sentence', 'for', 'sub', '##word', 'token', '##ization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Encoding\n",
        "\n",
        "* Words, or tokens, can't be processed by language models - they need to be converted into numbers.\n",
        "* We need a mapping from each token in out dictionary to a given id (number) - these numbers are what get processed by the AI."
      ],
      "metadata": {
        "id": "JfK5CfNO7OJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text and get input IDs\n",
        "batch_encoding: transformers.tokenization_utils_base.BatchEncoding = tokeniser(\n",
        "    [\n",
        "        text,\n",
        "        # \"this is another sentence\",\n",
        "    ],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        ")\n",
        "token_ids: torch.Tensor = batch_encoding[\"input_ids\"]  # Token IDs\n",
        "print(f\"{token_ids.shape=}\")\n",
        "print(f\"{token_ids=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR5--UO-7MrD",
        "outputId": "0ee17c62-f211-4fd4-f042-f8d5ba895e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_ids.shape=torch.Size([1, 13])\n",
            "token_ids=tensor([[  101,  2023,  2003,  2019,  2742,  6251,  2005,  4942, 18351, 19204,\n",
            "          3989,  1012,   102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T-JEPA\n",
        "\n",
        "See [Joint Embedding Prediictive Architrecture (JEPA)](https://arxiv.org/abs/2306.02572), [I-JEPA](https://arxiv.org/abs/2301.08243)."
      ],
      "metadata": {
        "id": "lKPWU_Kf75gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "sukatPrg7765"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENISATION\n",
        "vocab_size: int = tokeniser.vocab_size\n",
        "# TOKEN EMBEDDING\n",
        "embed_dim: int = 512  # 768  # BERT hidden dimension\n",
        "# CONTEXT/TARGET GENERATION\n",
        "target_scale_interval: Tuple[float, float] = (0.15, 0.2)\n",
        "context_scale_interval: Tuple[float, float] = (0.7, 0.9)\n",
        "# TRANSFORMER ENCODER/DECODER\n",
        "num_layers: int = 6  # Number of layers in the transformer\n",
        "num_heads: int = 8  # Number of attention heads in the transformer\n",
        "layer_dropout: float = 0.0\n",
        "\n",
        "print(f\"{vocab_size=}\")\n",
        "print(f\"{embed_dim=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiKXwx5c77Hn",
        "outputId": "3a04ed79-6057-46ee-9786-ba4753f83e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size=30522\n",
            "embed_dim=512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers"
      ],
      "metadata": {
        "id": "xLMTwQTT8a4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = nn.Embedding(\n",
        "    vocab_size, embed_dim\n",
        ") # Learns an `embed_dim`-dimensional (numerical) representation of each token in the vocabulary\n",
        "\n",
        "student_encoder = Encoder(\n",
        "    dim=embed_dim,\n",
        "    heads=num_heads,\n",
        "    depth=num_layers,\n",
        "    layer_dropout=layer_dropout,\n",
        ")\n",
        "\n",
        "teacher_encoder = copy.deepcopy(student_encoder)  # .cuda()  # copy student encoder\n",
        "\n",
        "decoder = Decoder(\n",
        "    dim=embed_dim,\n",
        "    depth=num_layers // 2,\n",
        "    heads=num_heads // 2,\n",
        "    layer_dropout=layer_dropout,\n",
        ")\n",
        "\n",
        "pos_embedding = ScaledSinusoidalEmbedding(embed_dim)"
      ],
      "metadata": {
        "id": "d7_di3Pn73r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "nn.init.trunc_normal_(mask_token, 0.02)\n",
        "None"
      ],
      "metadata": {
        "id": "b2EpVGaa8cmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed Token Ids\n",
        "\n",
        "Learn a numerical representation for the \"meaning\" of each token"
      ],
      "metadata": {
        "id": "aB1Jx0CB9k9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings: torch.Tensor = embedding_layer(token_ids)\n",
        "\n",
        "# Add positional embeddings\n",
        "# NOTE: Transformer networks have no positional awareness, so we need to tell the network the order of the tokens with positional embeddings\n",
        "token_embeddings = token_embeddings + pos_embedding(token_embeddings)\n",
        "print(f\"{token_embeddings.shape=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejg6lhEi8jr8",
        "outputId": "51e36ddf-37f3-411f-b4fa-c64b64a5c058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_embeddings.shape=torch.Size([1, 13, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers take in a fixed-length input, so we must pad short sentences with a \"pad\" token."
      ],
      "metadata": {
        "id": "DjBXbKiU-NIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokeniser.pad_token_id, tokeniser.pad_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPs9_e-t8lg8",
        "outputId": "73c1705e-a446-4ca4-bbcc-5943b4a8ea38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, '[PAD]')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Targets and Contexts\n",
        "\n",
        "In the JEPA framework, we mask the targets and use the contexts to reconstruct the targets."
      ],
      "metadata": {
        "id": "YE3P0hyR-Xqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_prob_range: Tuple[float, float] = (0.15, 0.35)\n",
        "\n",
        "target_prob: float = np.random.uniform(\n",
        "    low=target_prob_range[0], high=target_prob_range[1]\n",
        ")\n",
        "print(f\"{target_prob=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7uSJa-W8rEM",
        "outputId": "c8abe146-8d82-4278-a156-c5be965e01f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_prob=0.1684745906939956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_scale: float = np.random.uniform(\n",
        "    low=target_scale_interval[0], high=target_scale_interval[1]\n",
        ")\n",
        "context_scale: float = np.random.uniform(\n",
        "    low=context_scale_interval[0], high=context_scale_interval[1]\n",
        ")\n",
        "\n",
        "target_indices: torch.Tensor = torch.bernoulli(\n",
        "    torch.full(token_ids.shape, target_scale)  # target_probability_matrix\n",
        ").bool()\n",
        "context_indices: torch.Tensor = torch.bernoulli(\n",
        "    torch.full(token_ids.shape, context_scale)  # context_probability_matrix\n",
        ").bool()\n",
        "print(f\"{target_indices.shape=}\")\n",
        "print(f\"{target_indices=}\")\n",
        "print()\n",
        "print(f\"{context_indices.shape=}\")\n",
        "print(f\"{context_indices=}\")\n",
        "# NOTE: The targets and contexts are allowed to overlap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CotIgbsj8v_U",
        "outputId": "214eaff2-15b5-45cd-ee80-c94668cea691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_indices.shape=torch.Size([1, 13])\n",
            "target_indices=tensor([[False, False,  True, False,  True, False, False, False, False, False,\n",
            "         False, False, False]])\n",
            "\n",
            "context_indices.shape=torch.Size([1, 13])\n",
            "context_indices=tensor([[ True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_embeddings: torch.Tensor = token_embeddings[\n",
        "    None, target_indices\n",
        "]  # (batch_size, num_target_tokens, embed_dim)\n",
        "context_embeddings: torch.Tensor = token_embeddings[\n",
        "    None, context_indices\n",
        "]  # (batch_size, num_context_tokens, embed_dim)"
      ],
      "metadata": {
        "id": "B3g31tkL8zVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_encoding: torch.Tensor = teacher_encoder(\n",
        "    target_embeddings\n",
        ")  # (batch_size, num_target_tokens, embed_dim)\n",
        "context_encoding: torch.Tensor = student_encoder(\n",
        "    context_embeddings\n",
        ")  # (batch_size, num_context_tokens, embed_dim)\n",
        "target_embeddings.shape, context_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9KOf-rQ81FT",
        "outputId": "7a87e086-0b96-4296-f153-0585fc531b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 2, 512]), torch.Size([1, 12, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Reconstruction\n",
        "\n",
        "Use the context to predict the targets"
      ],
      "metadata": {
        "id": "aftnriEX83wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_dim, num_patches, _ = target_embeddings.shape\n",
        "target_masks: torch.Tensor = mask_token.repeat(batch_dim, num_patches, 1)\n",
        "print(f\"{target_masks.shape=}\")\n",
        "assert target_masks.shape == target_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvKKz3gE82Ij",
        "outputId": "3de9808e-25ac-483d-f82a-001bc7208d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_masks.shape=torch.Size([1, 2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Targets and contexts contain positional information\n",
        "# This positional information is un-affected by the concatenation\n",
        "x: torch.Tensor = torch.cat([context_embeddings, target_masks], dim=1)\n",
        "print(f\"{x.shape=}\")\n",
        "\n",
        "# Decode\n",
        "x = decoder(x) # Self-attention\n",
        "\n",
        "# Return the output corresponding to target tokens, i.e., the last len(target_masks) tokens\n",
        "prediction: torch.Tensor = x[:, -target_masks.shape[1] :, :]\n",
        "print(f\"{prediction.shape=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yttvfu5S850r",
        "outputId": "70cf014a-bd1d-4274-a7fd-3dc649c78f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape=torch.Size([1, 14, 512])\n",
            "prediction.shape=torch.Size([1, 2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Loss"
      ],
      "metadata": {
        "id": "OsfC9WCf-8Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss: torch.Tensor = criterion(prediction, target_embeddings)\n",
        "print(f\"{loss=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wca5xJj89u6",
        "outputId": "168374df-9116-48ca-987d-727e5d06a4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss=tensor(1.9379, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    }
  ]
}